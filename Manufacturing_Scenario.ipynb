{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Queue Monitoring System - Manufacturing Scenario\n",
    "\n",
    "## Overview\n",
    "Now that you have your Python script and job submission script, you're ready to request an edge node and run inference on the different hardware types (CPU, GPU, VPU, FPGA).\n",
    "\n",
    "After the inference is completed, the output video and stats files need to be retrieved and stored in the workspace, which can then be viewed within the Jupyter Notebook.\n",
    "\n",
    "## Objectives\n",
    "* Submit inference jobs to Intel's DevCloud using the `qsub` command.\n",
    "* Retrieve and review the results.\n",
    "* After testing, go back to the proposal doc and update your original proposed hardware device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Set Up\n",
    "\n",
    "#### IMPORTANT: Set up paths so we can run Dev Cloud utilities\n",
    "You *must* run this every time you enter a Workspace session.\n",
    "(Tip: select the cell and use **Shift+Enter** to run the cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support\n"
     ]
    }
   ],
   "source": [
    "%env PATH=/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/intel_devcloud_support\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel_devcloud_support'))\n",
    "sys.path.insert(0, os.path.abspath('/opt/intel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.1:  (Optional-step): Original Video\n",
    "\n",
    "If you are curious to see the input video, run the following cell to view the original video stream we'll be using for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h5>Manufacturing</h5>\n",
       "        <video alt=\"\" controls autoplay muted height=\"480\"><source src=\"original_videos/Manufacturing.mp4\" type=\"video/mp4\" /></video>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import videoHtml\n",
    "videoHtml.videoHTML('Manufacturing', ['original_videos/Manufacturing.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Inference on a Video\n",
    "\n",
    "In the next few cells, You'll submit your job using the `qsub` command and retrieve the results for each job. Each of the cells below should submit a job to different edge compute nodes.\n",
    "\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job with `liveQStat`.\n",
    "\n",
    "You will need to submit a job for each of the following hardware types:\n",
    "* **CPU**\n",
    "* **GPU**\n",
    "* **VPU**\n",
    "* **FPGA**\n",
    "\n",
    "**Note:** You will have to submit each job one at a time and retrieve their results. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(Tip: **shift+enter** will run the cell and automatically move you to the next cell.)\n",
    "\n",
    "If your job successfully runs and completes, once you retrieve your results, it should output a video and a stats text file in the `results/manufacturing/<DEVICE>` directory.\n",
    "\n",
    "For example, your **CPU** job should output its files in this directory:\n",
    "> **results/manufacturing/cpu**\n",
    "\n",
    "**Note**: To get the queue labels for the different hardware devices, you can go to [this link](https://devcloud.intel.com/edge/get_started/devcloud/).\n",
    "\n",
    "The following arguments should be passed to the job submission script after the `-F` flag:\n",
    "* Model path - `/data/models/intel/person-detection-retail-0013/<MODEL PRECISION>/`. You will need to adjust this path based on the model precision being using on the hardware.\n",
    "* Device - `CPU`, `GPU`, `MYRIAD`, `HETERO:FPGA,CPU`.\n",
    "* Manufacturing video path - `/data/resources/manufacturing.mp4`.\n",
    "* Manufacturing queue_param file path - `/data/queue_param/manufacturing.npy`.\n",
    "* Output path - `/output/results/manufacturing/<DEVICE>` This should be adjusted based on the device used in the job.\n",
    "* Max num of people - This is the max number of people in queue before the system would redirect them to another queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Submit to an Edge Compute Node with an Intel CPU\n",
    "In the cell below, write a script to submit a job to an edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Coreâ„¢ i5-6500TE processor</a>. The inference workload should run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U0lNT5A6fIgIVjeMUqSq38dPFkANLdvb\n"
     ]
    }
   ],
   "source": [
    "##Model_path = \"/data/models/intel/person-detection-retail-0013/FP32/person-detection-retail-0013\"\n",
    "##device = \"CPU\"\n",
    "##video_path = \"/data/resources/manufacturing.mp4\"\n",
    "##queue_param = \"/data/queue_param/manufacturing.npy\"\n",
    "##output_path = \"/output/results/manufacturing/cpu\"\n",
    "##max_people = \"5\"\n",
    "\n",
    "##args = \"{} {} {} {} {} {}\".format(Model_path, device, video_path, queue_param, output_path, max_people)\n",
    "\n",
    "#Submit job to the queue\n",
    "#cpu_job_id = !qsub queue_job.sh -d . -l nodes=1:tank-870:i5-6500te -F \"{args}\" -N manufacturing_cpu\n",
    "cpu_job_id = !qsub queue_job.sh -d . -l nodes=1:tank-870:i5-6500te -F \"/data/models/intel/person-detection-retail-0013/FP32/person-detection-retail-0013 CPU /data/resources/manufacturing.mp4 /data/queue_param/manufacturing.npy /output/results/manufacturing/cpu 5\"\n",
    "\n",
    "print(cpu_job_id[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Job Status\n",
    "\n",
    "To check on the job that was submitted, use `liveQStat` to check the status of the job.\n",
    "\n",
    "Column `S` shows the state of your running jobs.\n",
    "\n",
    "For example:\n",
    "- If `JOB ID`is in Q state, it is in the queue waiting for available resources.\n",
    "- If `JOB ID` is in R state, it is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liveQStat\n",
    "liveQStat.liveQStat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Results\n",
    "\n",
    "Run the next cell to retrieve your job's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getResults() is blocking until results of the job (id:U0lNT5A6fIgIVjeMUqSq38dPFkANLdvb) are ready.\n",
      "Please wait...Success!\n",
      "output.tgz was downloaded in the same folder as this notebook.\n"
     ]
    }
   ],
   "source": [
    "import get_results\n",
    "get_results.getResults(cpu_job_id[0], filename='output.tgz', blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unpack your output files and view stdout.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar zxf output.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/\r\n",
      "results/manufacturing/\r\n",
      "results/manufacturing/cpu/\r\n",
      "stderr.log\r\n"
     ]
    }
   ],
   "source": [
    "!cat stdout.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View stderr.log\n",
    "This can be used for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"person_detect.py\", line 50, in __init__\r\n",
      "    self.model=core.read_network(model=model_structure, weights=model_weights)\r\n",
      "NameError: name 'core' is not defined\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"person_detect.py\", line 199, in <module>\r\n",
      "    main(args)\r\n",
      "  File \"person_detect.py\", line 121, in main\r\n",
      "    pd= PersonDetect(model, device, threshold)\r\n",
      "  File \"person_detect.py\", line 52, in __init__\r\n",
      "    raise ValueError(\"Could not Initialise the network. Have you enterred the correct model path?\")\r\n",
      "ValueError: Could not Initialise the network. Have you enterred the correct model path?\r\n"
     ]
    }
   ],
   "source": [
    "!cat stderr.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Output Video\n",
    "Run the cell below to view the output video. If inference was successfully run, you should see a video with bounding boxes drawn around each person detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import videoHtml\n",
    "\n",
    "videoHtml.videoHTML('Manufacturing CPU', ['results/manufacturing/cpu/output_video.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Submit to an Edge Compute Node with CPU and IGPU\n",
    "In the cell below, write a script to submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">IntelÂ® Core i5-6500TE</a>. The inference workload should run on the **IntelÂ® HD Graphics 530** integrated GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Model_path = \"/data/models/intel/person-detection-retail-0013/FP16/person-detection-retail-0013\"\n",
    "device = \"GPU\"\n",
    "video_path = \"/data/resources/manufacturing.mp4\"\n",
    "queue_param = \"/data/queue_param/manufacturing.npy\"\n",
    "output_path = \"/output/results/manufacturing/gpu\"\n",
    "max_people = \"5\"\n",
    "\n",
    "args = \"{} {} {} {} {} {}\".format(Model_path, device, video_path, queue_param, output_path, max_people)\n",
    "\n",
    "\n",
    "#Submit job to the queue\n",
    "gpu_job_id = !qsub queue_job.sh -d . -l nodes=1:tank-870:i5-6500te:intel-hd-530 -F \"{args}\" -N manufacturing_gpu '''\n",
    "\n",
    "gpu_job_id = !qsub queue_job.sh -d . -l nodes=1:tank-870:i5-6500te:intel-hd-530 -F \"/data/models/intel/person-detection-retail-0013/FP16/person-detection-retail-0013 GPU /data/resources/manufacturing.mp4 /data/queue_param/manufacturing.npy /output/results/manufacturing/gpu 5\"\n",
    "\n",
    "\n",
    "\n",
    "print(gpu_job_id[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Job Status\n",
    "\n",
    "To check on the job that was submitted, use `liveQStat` to check the status of the job.\n",
    "\n",
    "Column `S` shows the state of your running jobs.\n",
    "\n",
    "For example:\n",
    "- If `JOB ID`is in Q state, it is in the queue waiting for available resources.\n",
    "- If `JOB ID` is in R state, it is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liveQStat\n",
    "liveQStat.liveQStat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Results\n",
    "\n",
    "Run the next cell to retrieve your job's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_results\n",
    "get_results.getResults(gpu_job_id[0], filename='output.tgz', blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unpack your output files and view stdout.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar zxf output.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat stdout.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View stderr.log\n",
    "This can be used for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat stderr.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Output Video\n",
    "Run the cell below to view the output video. If inference was successfully run, you should see a video with bounding boxes drawn around each person detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import videoHtml\n",
    "\n",
    "videoHtml.videoHTML('Manufacturing GPU', ['results/manufacturing/gpu/output_video.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3: Submit to an Edge Compute Node with a Neural Compute Stick 2\n",
    "In the cell below, write a script to submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload should run on an <a \n",
    "    href=\"https://software.intel.com/en-us/neural-compute-stick\">Intel Neural Compute Stick 2</a> installed in this  node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Model_path = \"/data/models/intel/person-detection-retail-0013/FP16/person-detection-retail-0013\"\n",
    "device = \"MYRIAD\"\n",
    "video_path = \"/data/resources/manufacturing.mp4\"\n",
    "queue_param = \"/data/queue_param/manufacturing.npy\"\n",
    "output_path = \"/output/results/manufacturing/vpu\"\n",
    "max_people = \"5\"\n",
    "\n",
    "args = \"{} {} {} {} {} {}\".format(Model_path, device, video_path, queue_param, output_path, max_people)\n",
    "\n",
    "\n",
    "#Submit job to the queue\n",
    "vpu_job_id = !qsub queue_job.sh -d . -l nodes=1:tank-870:i5-6500te:intel-ncs2 -F \"{args}\" -N manufacturing_vpu '''\n",
    "\n",
    "vpu_job_id = !qsub queue_job.sh -d . -l nodes=1:tank-870:i5-6500te:intel-ncs2 -F \"/data/models/intel/person-detection-retail-0013/FP16/person-detection-retail-0013 MYRIAD /data/resources/manufacturing.mp4 /data/queue_param/manufacturing.npy /output/results/manufacturing/vpu 5\"\n",
    "print(vpu_job_id[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Job Status\n",
    "\n",
    "To check on the job that was submitted, use `liveQStat` to check the status of the job.\n",
    "\n",
    "Column `S` shows the state of your running jobs.\n",
    "\n",
    "For example:\n",
    "- If `JOB ID`is in Q state, it is in the queue waiting for available resources.\n",
    "- If `JOB ID` is in R state, it is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liveQStat\n",
    "liveQStat.liveQStat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Results\n",
    "\n",
    "Run the next cell to retrieve your job's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_results\n",
    "get_results.getResults(vpu_job_id[0], filename='output.tgz', blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unpack your output files and view stdout.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar zxf output.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat stdout.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View stderr.log\n",
    "This can be used for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat stderr.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Output Video\n",
    "Run the cell below to view the output video. If inference was successfully run, you should see a video with bounding boxes drawn around each person detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import videoHtml\n",
    "\n",
    "videoHtml.videoHTML('Manufacturing VPU', ['results/manufacturing/vpu/output_video.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4: Submit to an Edge Compute Node with IEI Mustang-F100-A10\n",
    "In the cell below, write a script to submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Coreâ„¢ i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\"> IEI Mustang-F100-A10 </a> FPGA card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Model_path = \"/data/models/intel/person-detection-retail-0013/FP32/person-detection-retail-0013\"\n",
    "device = \"HETERO:FPGA,CPU\"\n",
    "video_path = \"/data/resources/manufacturing.mp4\"\n",
    "queue_param = \"/data/queue_param/manufacturing.npy\"\n",
    "output_path = \"/output/results/manufacturing/fpga\"\n",
    "max_people = \"5\"\n",
    "\n",
    "args = \"{} {} {} {} {} {}\".format(Model_path, device, video_path, queue_param, output_path, max_people)\n",
    "\n",
    "#Submit job to the queue\n",
    "fpga_job_id = !qsub queue_job.sh -d . -l nodes=1:tank-870:i5-6500te:iei-mustang-f100-a10 -F \"{args}\" -N manufacturing_fpga '''\n",
    "\n",
    "fpga_job_id =  !qsub queue_job.sh -d . -l nodes=1:tank-870:i5-6500te:iei-mustang-f100-a10 -F \"/data/models/intel/person-detection-retail-0013/FP16/person-detection-retail-0013 HETERO:FPGA,CPU /data/resources/manufacturing.mp4 /data/queue_param/manufacturing.npy /output/results/manufacturing/fpga 5\"\n",
    "\n",
    "print(fpga_job_id[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Job Status\n",
    "\n",
    "To check on the job that was submitted, use `liveQStat` to check the status of the job.\n",
    "\n",
    "Column `S` shows the state of your running jobs.\n",
    "\n",
    "For example:\n",
    "- If `JOB ID`is in Q state, it is in the queue waiting for available resources.\n",
    "- If `JOB ID` is in R state, it is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liveQStat\n",
    "liveQStat.liveQStat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Results\n",
    "\n",
    "Run the next cell to retrieve your job's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_results\n",
    "get_results.getResults(fpga_job_id[0], filename='output.tgz', blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unpack your output files and view stdout.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar zxf output.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat stdout.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View stderr.log\n",
    "This can be used for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat stderr.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Output Video\n",
    "Run the cell below to view the output video. If inference was successfully run, you should see a video with bounding boxes drawn around each person detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import videoHtml\n",
    "\n",
    "videoHtml.videoHTML('Manufacturing FPGA', ['results/manufacturing/fpga/output_video.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Wait!***\n",
    "\n",
    "Please wait for all the inference jobs and video rendering to complete before proceeding to the next step.\n",
    "\n",
    "## Step 2: Assess Performance\n",
    "\n",
    "Run the cells below to compare the performance across all 4 devices. The following timings for the model are being compared across all 4 devices:\n",
    "\n",
    "- Model Loading Time\n",
    "- Average Inference Time\n",
    "- FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device_list=['cpu', 'gpu', 'fpga', 'vpu']\n",
    "inference_time=[]\n",
    "fps=[]\n",
    "model_load_time=[]\n",
    "\n",
    "for device in device_list:\n",
    "    with open('results/manufacturing/'+device+'/stats.txt', 'r') as f:\n",
    "        inference_time.append(float(f.readline().split(\"\\n\")[0]))\n",
    "        fps.append(float(f.readline().split(\"\\n\")[0]))\n",
    "        model_load_time.append(float(f.readline().split(\"\\n\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(device_list, inference_time)\n",
    "plt.xlabel(\"Device Used\")\n",
    "plt.ylabel(\"Total Inference Time in Seconds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(device_list, fps)\n",
    "plt.xlabel(\"Device Used\")\n",
    "plt.ylabel(\"Frames per Second\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(device_list, model_load_time)\n",
    "plt.xlabel(\"Device Used\")\n",
    "plt.ylabel(\"Model Loading Time in Seconds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Update Proposal Document\n",
    "\n",
    "Now that you've completed your hardware testing, you should go back to the proposal document and validate or update your originally proposed hardware. Once you've updated your proposal, you can move on to the next scenario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
